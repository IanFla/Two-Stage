{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "from pandas_datareader import DataReader as DR\n",
    "import seaborn as sb\n",
    "import numdifftools as nd\n",
    "from wquantiles import quantile\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from scipy.stats import norm,t,truncnorm\n",
    "from scipy.stats import multivariate_normal as mvnorm\n",
    "from scipy.stats import multivariate_t as mvt\n",
    "from scipy.spatial import Delaunay as TRI\n",
    "from scipy.interpolate import LinearNDInterpolator as ITP\n",
    "from scipy.optimize import minimize,root\n",
    "from scipy.optimize import NonlinearConstraint as NonlinCons\n",
    "from scipy.stats import gaussian_kde as sciKDE\n",
    "\n",
    "from sklearn.linear_model import LinearRegression as Linear\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.neighbors import KernelDensity as sklKDE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the experiment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLE:\n",
    "    def __init__(self,dim,sigma):\n",
    "        self.dim=dim\n",
    "        self.T=lambda x: mvnorm.pdf(x=x,mean=np.zeros(dim))\n",
    "        self.iP=lambda x: mvnorm.pdf(x=x,mean=np.zeros(dim),cov=sigma**2)\n",
    "        self.iS=lambda size: mvnorm.rvs(size=size,mean=np.zeros(dim),cov=sigma**2)\n",
    "        \n",
    "    def __estimate(self,W,name,asym=True):\n",
    "        Z=np.mean(W)\n",
    "        err=np.abs(Z-1)\n",
    "        if asym:\n",
    "            aVar=np.var(W)\n",
    "            aErr=np.sqrt(aVar/W.size)\n",
    "            ESS=1/np.sum((W/np.sum(W))**2)\n",
    "            print('{} est: {:.4f}; err: {:.4f}; a-var: {:.4f}; a-err: {:.4f}; ESS: {:.0f}/{}'\\\n",
    "                  .format(name,Z,err,aVar,aErr,ESS,W.size))\n",
    "        else:\n",
    "            print('{} est: {:.4f}; err: {:.4f}'.format(name,Z,err))\n",
    "        \n",
    "    def estimate_IS(self,size):\n",
    "        S=self.iS(size)\n",
    "        W=self.T(S)/self.iP(S)\n",
    "        self.__estimate(W,'IS')\n",
    "    \n",
    "    def draw_TP(self,P,x,name,dim=0):\n",
    "        X=np.zeros([x.size,self.dim])\n",
    "        X[:,dim]=x\n",
    "        ratio=np.reshape(self.T(np.zeros(self.dim))/P(np.zeros([1,self.dim])),1)[0]\n",
    "        print('------------ pdf ratio at origin: {:.2f} ------------'.format(ratio))\n",
    "        \n",
    "        fig,ax=plt.subplots(figsize=(7,4))\n",
    "        ax.plot(x,self.T(X))\n",
    "        ax.plot(x,P(X))\n",
    "        if name=='nonparametric':\n",
    "            one=np.zeros(self.dim)\n",
    "            one[dim]=1\n",
    "            rW=np.array([self.h(one*loc,loc) for loc in self.rS])\n",
    "            rW=rW/rW.max()*P(np.zeros([1,self.dim]))[0]\n",
    "            rWmeans=np.ones_like(rW)*rW.mean()\n",
    "\n",
    "            ax.plot(x,self.mP(X))\n",
    "            ax.hist(self.rS[:,dim],bins=2*rW.size,weights=rWmeans)\n",
    "            ax.hist(self.rS[:,dim],bins=2*rW.size,weights=rW)\n",
    "            ax.legend(['target','nonparametric proposal','mixture proposal','centers','centers with weight'])\n",
    "        elif name=='regression':\n",
    "            G=self.G(X)\n",
    "            rPO=self.regO.coef_.dot(G)+self.regO.intercept_*P(X)\n",
    "            rPL=self.regL.coef_.dot(G)+self.regL.intercept_*P(X)\n",
    "            mid=int(x.size/2)\n",
    "            print('regression ratios: ordinary {:.4f}, lasso {:.4f}'\\\n",
    "                  .format(self.T(X[mid])/rPO[mid],self.T(X[mid])/rPL[mid]))\n",
    "            \n",
    "            ax.plot(x,rPO)\n",
    "            ax.plot(x,rPL)\n",
    "            ax.legend(['target','mixture proposal','ordinary regression','lasso regression'])\n",
    "        else:\n",
    "            ax.legend(['target','{} proposal'.format(name)])\n",
    "            \n",
    "        ax.set_title('{}-D target and {} proposal (cross-sectional view)'.format(self.dim,name))\n",
    "        plt.show()\n",
    "        \n",
    "    def resample(self,size,ratio):\n",
    "        S=self.iS(ratio*size)\n",
    "        p=self.T(S)/self.iP(S)\n",
    "        index=np.arange(S.shape[0])\n",
    "        choice=np.random.choice(index,size,p=p/np.sum(p),replace=True)\n",
    "        \n",
    "        self.rS=S[choice]\n",
    "        self.rSset=S[list(set(choice))]\n",
    "        print('resampling rate: {}/{}'.format(self.rSset.shape[0],size))\n",
    "        \n",
    "    def estimate_NIS(self,size,rate,bdwth='scott'):\n",
    "        if(type(bdwth)==str):\n",
    "            method=bdwth\n",
    "            tmp=sciKDE(self.rS.T,bw_method=method)\n",
    "            bdwth=np.mean(np.sqrt(np.diag(tmp.covariance_factor()*np.cov(self.rS.T))))\n",
    "            print('bdwth: {:.4f} (based on {})'.format(bdwth,method))\n",
    "        \n",
    "        self.bdwth=bdwth\n",
    "        self.kde=sklKDE(kernel='gaussian',bandwidth=bdwth).fit(self.rS)\n",
    "        self.h=lambda x,loc: mvnorm.pdf(x=x,mean=loc,cov=self.bdwth**2)\n",
    "        self.G=lambda x: np.array([self.h(x,loc) for loc in self.rSset])-self.iP(x)\n",
    "        \n",
    "        self.nP=lambda x: np.exp(self.kde.score_samples(x))\n",
    "        self.nS=lambda size: self.kde.sample(size)\n",
    "        S=self.nS(size)\n",
    "        W=self.T(S)/self.nP(S)\n",
    "        self.__estimate(W,'NIS')\n",
    "        \n",
    "        self.mP=lambda x: (1-rate)*self.iP(x)+rate*self.nP(x)\n",
    "        self.mS=lambda size: np.vstack([self.iS(size-round(rate*size)),self.nS(round(rate*size))])\n",
    "        self.S=self.mS(size)\n",
    "        W=self.T(self.S)/self.mP(self.S)\n",
    "        self.__estimate(W,'MIS')\n",
    "        \n",
    "    def estimate_RIS(self,alpha):\n",
    "        X=(self.G(self.S)/self.mP(self.S)).T\n",
    "        y=self.T(self.S)/self.mP(self.S)\n",
    "        self.regO=Linear().fit(X,y)\n",
    "        self.regL=Lasso(alpha).fit(X,y)\n",
    "        print('Ordinary R2: {:.4f}; Lasso R2: {:.4f}'.format(self.regO.score(X,y),self.regL.score(X,y)))\n",
    "        \n",
    "        W=y-X.dot(self.regO.coef_)\n",
    "        self.__estimate(W,'RIS(Ord)')\n",
    "        W=y-X.dot(self.regL.coef_)\n",
    "        self.__estimate(W,'RIS(Las)')\n",
    "    \n",
    "    def estimate_MLE(self,opt=False,init=0):\n",
    "        mP=self.mP(self.S)\n",
    "        G=self.G(self.S)\n",
    "        target=lambda zeta: -np.mean(np.log(mP+zeta.dot(G)))\n",
    "        gradient=lambda zeta: -np.mean(G/(mP+zeta.dot(G)),axis=1)\n",
    "        hessian=lambda zeta: (G/(mP+zeta.dot(G))**2).dot(G.T)/G.shape[1]\n",
    "        zeta0=np.zeros(G.shape[0])\n",
    "        grad0=gradient(zeta0)\n",
    "        print('Reference:')\n",
    "        print('origin: value: {:.4f}; grad: (min {:.4f}, mean {:.4f}, max {:.4f}, std {:.4f})'\\\n",
    "              .format(target(zeta0),grad0.min(),grad0.mean(),grad0.max(),grad0.std()))\n",
    "        \n",
    "        print()\n",
    "        print('Theoretical results:')\n",
    "        X=(G/mP).T\n",
    "        XX=X-X.mean(axis=0)\n",
    "        zeta1=np.linalg.solve(XX.T.dot(XX),X.sum(axis=0))\n",
    "        print('MLE(The) zeta: (min {:.4f}, mean {:.4f}, max {:.4f}, std {:.4f}, norm {:.4f})'\\\n",
    "              .format(zeta1.min(),zeta1.mean(),zeta1.max(),zeta1.std(),np.sqrt(np.sum(zeta1**2))))\n",
    "        grad1=gradient(zeta1)\n",
    "        print('theory: value: {:.4f}; grad: (min {:.4f}, mean {:.4f}, max {:.4f}, std {:.4f})'\\\n",
    "              .format(target(zeta1),grad1.min(),grad1.mean(),grad1.max(),grad1.std()))\n",
    "        W=(self.T(self.S)/mP)*(1-XX.dot(zeta1))\n",
    "        self.__estimate(W,'RIS(The)',asym=False)\n",
    "        W=self.T(self.S)/(mP+zeta1.dot(G))\n",
    "        self.__estimate(W,'MLE(The)',asym=False)\n",
    "        \n",
    "        if opt:\n",
    "            zeta=zeta1 if init==1 else zeta0\n",
    "            begin=dt.now()\n",
    "            res=root(lambda zeta: (gradient(zeta),hessian(zeta)),zeta,method='lm',jac=True)\n",
    "            end=dt.now()\n",
    "            print()\n",
    "            print('Optimization results (spent {} seconds):'.format((end-begin).seconds))\n",
    "            if res['success']:\n",
    "                zeta=res['x']\n",
    "                print('MLE(Opt) zeta: (min {:.4f}, mean {:.4f}, max {:.4f}, std {:.4f}, norm {:.4f})'\\\n",
    "                      .format(zeta.min(),zeta.mean(),zeta.max(),zeta.std(),np.sqrt(np.sum(zeta**2))))\n",
    "                print('Dist(zeta(Opt),zeta(The))={:.4f}'.format(np.sqrt(np.sum((zeta-zeta1)**2))))\n",
    "                grad=gradient(zeta)\n",
    "                print('optimal: value: {:.4f}; grad: (min {:.4f}, mean {:.4f}, max {:.4f}, std {:.4f})'\\\n",
    "                      .format(target(zeta),grad.min(),grad.mean(),grad.max(),grad.std()))\n",
    "                W=self.T(self.S)/(mP+zeta.dot(G))\n",
    "                self.__estimate(W,'MLE(Opt)',asym=False)\n",
    "            else:\n",
    "                print('MLE fail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations:**\n",
    "1. didn't consider self-normalized importance sampling\n",
    "2. only for symmetric normal target and symmetric normal initial proposal with only one mode\n",
    "3. only for normal KDE without weights and adaptive bandwidth\n",
    "4. only consider regression for proposal components based on mixture proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial proposal and the curse of dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle=MLE(dim=2,sigma=2)\n",
    "size=1000000\n",
    "mle.estimate_IS(size)\n",
    "x=np.linspace(-4,4,101)\n",
    "mle.draw_TP(mle.iP,x,'initial')\n",
    "\n",
    "print('=======================================================')\n",
    "print('+++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "print('=======================================================')\n",
    "\n",
    "mle=MLE(dim=4,sigma=2)\n",
    "mle.estimate_IS(size)\n",
    "mle.draw_TP(mle.iP,x,'initial')\n",
    "\n",
    "print('=======================================================')\n",
    "print('+++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "print('=======================================================')\n",
    "\n",
    "mle=MLE(dim=8,sigma=2)\n",
    "mle.estimate_IS(size)\n",
    "mle.draw_TP(mle.iP,x,'initial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "* The cumulative phenomenon of scale difference of all dimension. \n",
    "\n",
    "**Future:**\n",
    "* What about the comparison between multi-normal and multi-t? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KDE with different initial proposal and bandwidth in 6-dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "mle=MLE(dim=6,sigma=1)\n",
    "size=50000\n",
    "mle.estimate_IS(size)\n",
    "x=np.linspace(-4,4,101)\n",
    "mle.draw_TP(mle.iP,x,'initial')\n",
    "print('=======================================================')\n",
    "mle.resample(1000,20)\n",
    "mle.estimate_NIS(size,0.9)\n",
    "mle.draw_TP(mle.nP,x,'nonparametric')\n",
    "\n",
    "print('=======================================================')\n",
    "print('+++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "print('=======================================================')\n",
    "\n",
    "np.random.seed(1234)\n",
    "mle=MLE(dim=6,sigma=2)\n",
    "mle.estimate_IS(size)\n",
    "mle.draw_TP(mle.iP,x,'initial')\n",
    "print('=======================================================')\n",
    "mle.resample(1000,20)\n",
    "np.random.seed(123456)\n",
    "mle.estimate_NIS(size,0.9)\n",
    "mle.draw_TP(mle.nP,x,'nonparametric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456)\n",
    "mle.estimate_NIS(size,0.9,0.6)\n",
    "mle.draw_TP(mle.nP,x,'nonparametric')\n",
    "\n",
    "print('=======================================================')\n",
    "print('+++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "print('=======================================================')\n",
    "\n",
    "np.random.seed(123456)\n",
    "mle.estimate_NIS(size,0.9,0.5)\n",
    "mle.draw_TP(mle.nP,x,'nonparametric')\n",
    "\n",
    "print('=======================================================')\n",
    "print('+++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "print('=======================================================')\n",
    "\n",
    "np.random.seed(123456)\n",
    "mle.estimate_NIS(size,0.9,0.4)\n",
    "mle.draw_TP(mle.nP,x,'nonparametric')\n",
    "\n",
    "print('=======================================================')\n",
    "print('+++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "print('=======================================================')\n",
    "\n",
    "np.random.seed(123456)\n",
    "mle.estimate_NIS(size,0.8,0.4)\n",
    "mle.draw_TP(mle.nP,x,'nonparametric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "* Resampling ratio 20:1 can give us good 'samples' from the target. (We can increase it for better resample rate in higher dimension. )\n",
    "* Only conservative bandwidth can work well in higher dimension, because little bandwidth can raise serious tail problem. \n",
    "\n",
    "**Future**\n",
    "* What about KDE with weights? \n",
    "* What about the adaptive bandwidth? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression performance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "mle=MLE(dim=8,sigma=2)\n",
    "size=100000\n",
    "mle.estimate_IS(size)\n",
    "x=np.linspace(-4,4,101)\n",
    "mle.draw_TP(mle.iP,x,'initial')\n",
    "print('=======================================================')\n",
    "mle.resample(1000,20)\n",
    "mle.estimate_NIS(size,0.9)\n",
    "mle.draw_TP(mle.nP,x,'nonparametric')\n",
    "print('=======================================================')\n",
    "mle.estimate_RIS(0.1)\n",
    "mle.draw_TP(mle.mP,x,'regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "mle=MLE(dim=10,sigma=2)\n",
    "mle.estimate_IS(size)\n",
    "mle.draw_TP(mle.iP,x,'initial')\n",
    "print('=======================================================')\n",
    "np.random.seed(123456)\n",
    "mle.resample(1000,20)\n",
    "mle.estimate_NIS(size,0.9)\n",
    "mle.draw_TP(mle.nP,x,'nonparametric')\n",
    "print('=======================================================')\n",
    "mle.estimate_RIS(0.1)\n",
    "mle.draw_TP(mle.mP,x,'regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456)\n",
    "mle.resample(1000,100)\n",
    "mle.estimate_NIS(size,0.9)\n",
    "mle.draw_TP(mle.nP,x,'nonparametric')\n",
    "print('=======================================================')\n",
    "mle.estimate_RIS(0.1)\n",
    "mle.draw_TP(mle.mP,x,'regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "* Regression improve the estimation performance by mimic the shape of the target. \n",
    "* In higher dimension, improve the quality of kernels by increasing resampling ratio can slightly improve KDE but more dramatically improve regression. \n",
    "\n",
    "**Future**\n",
    "* The problem of singularity? (What about multi-t initial? )\n",
    "* What about regression for kernels not used in KDE? \n",
    "* What about regression with KDE with weights? \n",
    "* What about regression for kernels with adaptive bandwidth? (Will that improve the kernels' quality? )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE method investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "mle=MLE(dim=10,sigma=2)\n",
    "size=100000\n",
    "mle.estimate_IS(size)\n",
    "x=np.linspace(-4,4,101)\n",
    "mle.draw_TP(mle.iP,x,'initial')\n",
    "print('=======================================================')\n",
    "mle.resample(1000,100)\n",
    "mle.estimate_NIS(size,0.9)\n",
    "mle.draw_TP(mle.nP,x,'nonparametric')\n",
    "print('=======================================================')\n",
    "mle.estimate_RIS(0.1)\n",
    "mle.draw_TP(mle.mP,x,'regression')\n",
    "print('=======================================================')\n",
    "mle.estimate_MLE(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "mle=MLE(dim=10,sigma=1)\n",
    "mle.estimate_IS(size)\n",
    "mle.draw_TP(mle.iP,x,'initial')\n",
    "print('=======================================================')\n",
    "mle.resample(1000,100)\n",
    "mle.estimate_NIS(size,0.9)\n",
    "mle.draw_TP(mle.nP,x,'nonparametric')\n",
    "print('=======================================================')\n",
    "mle.estimate_RIS(0.1)\n",
    "mle.draw_TP(mle.mP,x,'regression')\n",
    "print('=======================================================')\n",
    "mle.estimate_MLE(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "mle=MLE(dim=5,sigma=2)\n",
    "size=20000\n",
    "mle.estimate_IS(size)\n",
    "x=np.linspace(-4,4,101)\n",
    "mle.draw_TP(mle.iP,x,'initial')\n",
    "print('=======================================================')\n",
    "mle.resample(1000,100)\n",
    "mle.estimate_NIS(size,0.9)\n",
    "mle.draw_TP(mle.nP,x,'nonparametric')\n",
    "print('=======================================================')\n",
    "mle.estimate_RIS(0.1)\n",
    "mle.draw_TP(mle.mP,x,'regression')\n",
    "print('=======================================================')\n",
    "mle.estimate_MLE(True,0)\n",
    "\n",
    "print('=======================================================')\n",
    "print('+++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "print('=======================================================')\n",
    "\n",
    "mle.estimate_MLE(True,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "* Theoretical zeta is very close to optimal zeta, so we can use it as the initial guess for optimization. \n",
    "* Root finding for the gradient is far more efficient than maximization. \n",
    "* Theoretical RIS give exact same result as regression. \n",
    "* Theoretical MLE can achieve similar results as MLE based on optimization, so we can use it as an alternative method. \n",
    "* Theoretical MLE can't give exact result when target can be expressed by proposals, because its gradients are not small enough. \n",
    "* Sometimes (low-dimension, many kernels and small sample size? ) the theoretical zeta can be out of the feasible region, which indicates that the optimal zeta is very close to the boundary. \n",
    "* A bad theoretical zeta still is a good initial guess for optimization, which may result in infeasible but usable optimization result. \n",
    "\n",
    "**Future**\n",
    "* Try more detailed experiments for MLE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
